{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GF7KL8HBPppa"
      },
      "outputs": [],
      "source": [
        "!pip install -q pandas pyarrow langchain langchain-text-splitters tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "3IR-a18SQEKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1fb67e-1a12-47f6-8a5b-d398c97ce6e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Root directory to search (Adjust if your folder structure is different)\n",
        "SEARCH_ROOT = \"/content/drive/MyDrive/data_clean\"\n",
        "\n",
        "def find_directory(root_dir, target_name):\n",
        "    \"\"\"Recursively finds a directory with the given name.\"\"\"\n",
        "    print(f\"Searching for '{target_name}' in {root_dir}...\")\n",
        "    for root, dirs, files in os.walk(root_dir):\n",
        "        if target_name in dirs:\n",
        "            return os.path.join(root, target_name)\n",
        "    return None\n",
        "\n",
        "# Find the specific subdirectories we need\n",
        "print(\"Creating paths...\")\n",
        "\n",
        "# Find 'questions' folder (it might be inside another data_clean folder)\n",
        "questions_root = find_directory(SEARCH_ROOT, \"questions\")\n",
        "if questions_root:\n",
        "    QUESTIONS_DIR = os.path.join(questions_root, \"US\")\n",
        "    print(f\"✅ Found Questions Directory: {QUESTIONS_DIR}\")\n",
        "else:\n",
        "    print(\"❌ Could not find 'questions' directory. Please check your Drive structure.\")\n",
        "    QUESTIONS_DIR = None\n",
        "\n",
        "# Find 'textbooks' folder\n",
        "textbooks_root = find_directory(SEARCH_ROOT, \"textbooks\")\n",
        "if textbooks_root:\n",
        "    TEXTBOOKS_DIR = os.path.join(textbooks_root, \"en\")\n",
        "    print(f\"✅ Found Textbooks Directory: {TEXTBOOKS_DIR}\")\n",
        "else:\n",
        "    print(\"❌ Could not find 'textbooks' directory.\")\n",
        "    TEXTBOOKS_DIR = None\n",
        "\n",
        "OUTPUT_DIR = \"/content/processed_data\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stSCsDgExcQr",
        "outputId": "7de55a78-2982-4204-d5ed-828b3de56cc1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating paths...\n",
            "Searching for 'questions' in /content/drive/MyDrive/data_clean...\n",
            "✅ Found Questions Directory: /content/drive/MyDrive/data_clean/data_clean/questions/US\n",
            "Searching for 'textbooks' in /content/drive/MyDrive/data_clean...\n",
            "✅ Found Textbooks Directory: /content/drive/MyDrive/data_clean/data_clean/textbooks/en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "def format_question_for_finetuning(record):\n",
        "    \"\"\"\n",
        "    Formats a single QA record into an instruction prompt.\n",
        "    \"\"\"\n",
        "    question = record['question']\n",
        "    options = record['options']\n",
        "    answer_idx = record['answer_idx']\n",
        "    answer_text = record['answer']\n",
        "\n",
        "    # Format options nicely\n",
        "    options_str = \"\\n\".join([f\"{k}: {v}\" for k, v in options.items()])\n",
        "\n",
        "    # Construct the Input (Instruction)\n",
        "    instruction = f\"Answer the following multiple-choice question about medicine.\\n\\nQuestion:\\n{question}\\n\\nOptions:\\n{options_str}\"\n",
        "\n",
        "    # Construct the Output (Response)\n",
        "    response = f\"The correct answer is {answer_idx}. {answer_text}\"\n",
        "\n",
        "    return {\n",
        "        \"instruction\": instruction,\n",
        "        \"input\": \"\", # No extra input context needed here\n",
        "        \"output\": response\n",
        "    }\n",
        "\n",
        "def process_qa_files(input_dir, output_file):\n",
        "    if not input_dir or not os.path.exists(input_dir):\n",
        "        print(f\"Error: Input directory '{input_dir}' does not exist.\")\n",
        "        return []\n",
        "\n",
        "    all_records = []\n",
        "    # Process train, dev, and test files\n",
        "    for split in ['train', 'dev', 'test']:\n",
        "        file_path = os.path.join(input_dir, f\"{split}.jsonl\")\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Warning: {file_path} not found.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing {split} set...\")\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    data = json.loads(line)\n",
        "                    processed = format_question_for_finetuning(data)\n",
        "                    all_records.append(processed)\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Skipping invalid line in {file_path}\")\n",
        "\n",
        "    if not all_records:\n",
        "        print(\"No records found! Check if the .jsonl files exist in the directory.\")\n",
        "        return []\n",
        "\n",
        "    # Save as JSONL for training\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for record in all_records:\n",
        "            f.write(json.dumps(record) + \"\\n\")\n",
        "\n",
        "    print(f\"Saved {len(all_records)} processed records to {output_file}\")\n",
        "    return all_records\n",
        "\n",
        "# Run the processing if directory was found\n",
        "if QUESTIONS_DIR:\n",
        "    qa_output_path = os.path.join(OUTPUT_DIR, \"medqa_finetune_data.jsonl\")\n",
        "    processed_data = process_qa_files(QUESTIONS_DIR, qa_output_path)\n",
        "\n",
        "    # Show a sample if data exists\n",
        "    if processed_data:\n",
        "        print(\"\\nSample Data Point:\")\n",
        "        print(json.dumps(processed_data[0], indent=2))\n",
        "else:\n",
        "    print(\"Skipping QA processing because Questions directory was not found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIexual8yV99",
        "outputId": "6ead6344-038b-44a0-9ae1-2210f1c71c8e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing train set...\n",
            "Processing dev set...\n",
            "Processing test set...\n",
            "Saved 12723 processed records to /content/processed_data/medqa_finetune_data.jsonl\n",
            "\n",
            "Sample Data Point:\n",
            "{\n",
            "  \"instruction\": \"Answer the following multiple-choice question about medicine.\\n\\nQuestion:\\nA 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7\\u00b0F (36.5\\u00b0C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\\n\\nOptions:\\nA: Ampicillin\\nB: Ceftriaxone\\nC: Ciprofloxacin\\nD: Doxycycline\\nE: Nitrofurantoin\",\n",
            "  \"input\": \"\",\n",
            "  \"output\": \"The correct answer is E. Nitrofurantoin\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from pathlib import Path\n",
        "\n",
        "def process_textbooks(input_dir, output_file):\n",
        "    if not input_dir or not os.path.exists(input_dir):\n",
        "        print(f\"Error: Textbooks directory '{input_dir}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    text_files = glob.glob(os.path.join(input_dir, \"*.txt\"))\n",
        "    print(f\"Found {len(text_files)} textbooks.\")\n",
        "\n",
        "    if not text_files:\n",
        "        print(\"No .txt files found in the directory.\")\n",
        "        return\n",
        "\n",
        "    # Configurable chunking strategy\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    all_chunks = []\n",
        "\n",
        "    for file_path in text_files:\n",
        "        book_name = Path(file_path).stem\n",
        "        print(f\"Processing {book_name}...\")\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                text = f.read()\n",
        "\n",
        "            chunks = text_splitter.create_documents([text])\n",
        "\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                all_chunks.append({\n",
        "                    \"id\": f\"{book_name}_{i}\",\n",
        "                    \"text\": chunk.page_content,\n",
        "                    \"source\": book_name,\n",
        "                    \"chunk_index\": i\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "    # Save chunks\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_chunks, f, indent=2)\n",
        "\n",
        "    print(f\"Saved {len(all_chunks)} chunks to {output_file}\")\n",
        "\n",
        "# Run the chunking if directory was found\n",
        "if TEXTBOOKS_DIR:\n",
        "    chunks_output_path = os.path.join(OUTPUT_DIR, \"textbook_chunks.json\")\n",
        "    process_textbooks(TEXTBOOKS_DIR, chunks_output_path)\n",
        "else:\n",
        "    print(\"Skipping Textbook processing because Textbooks directory was not found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaYRHx7byb_e",
        "outputId": "ba512b14-b6cc-4935-dbbe-d0e122ab4533"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 18 textbooks.\n",
            "Processing Gynecology_Novak...\n",
            "Processing Biochemistry_Lippincott...\n",
            "Processing Physiology_Levy...\n",
            "Processing Obstentrics_Williams...\n",
            "Processing Neurology_Adams...\n",
            "Processing InternalMed_Harrison...\n",
            "Processing Surgery_Schwartz...\n",
            "Processing Pathology_Robbins...\n",
            "Processing Immunology_Janeway...\n",
            "Processing Cell_Biology_Alberts...\n",
            "Processing Anatomy_Gray...\n",
            "Processing Psichiatry_DSM-5...\n",
            "Processing First_Aid_Step2...\n",
            "Processing Histology_Ross...\n",
            "Processing Pathoma_Husain...\n",
            "Processing Pediatrics_Nelson...\n",
            "Processing Pharmacology_Katzung...\n",
            "Processing First_Aid_Step1...\n",
            "Saved 126803 chunks to /content/processed_data/textbook_chunks.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "if os.path.exists(qa_output_path):\n",
        "    files.download(qa_output_path)\n",
        "if os.path.exists(chunks_output_path):\n",
        "    files.download(chunks_output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "KiLJuEpuy16S",
        "outputId": "da53487e-70a7-418a-e17e-0735d7504453"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0a1b4d65-5cc5-4529-a2a3-8550261c12c1\", \"medqa_finetune_data.jsonl\", 13761142)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_212b2770-771e-4369-8617-8e0c7a243e00\", \"textbook_chunks.json\", 111528526)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}